{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is normal SNN network without stashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "beta = 0.95\n",
    "# The beta defined here = (1 - delta(t)/Tau)\n",
    "# Here delta(t) is the clk period\n",
    "# Tau = Time constant = RC\n",
    "# This is the approximation of beta = exp(-delta(t)/Tau)\n",
    "num_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform learning over cuda\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./../data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(\"./../data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and test data\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are creating a single hidden layer NN, with 150 nodes\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(784, 150)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(150, 10)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(net.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step[100/937], Loss: 24.5360\n",
      "Epoch [1/5], Step[200/937], Loss: 23.1167\n",
      "Epoch [1/5], Step[300/937], Loss: 21.7091\n",
      "Epoch [1/5], Step[400/937], Loss: 10.9026\n",
      "Epoch [1/5], Step[500/937], Loss: 13.8067\n",
      "Epoch [1/5], Step[600/937], Loss: 10.4567\n",
      "Epoch [1/5], Step[700/937], Loss: 9.8155\n",
      "Epoch [1/5], Step[800/937], Loss: 8.3033\n",
      "Epoch [1/5], Step[900/937], Loss: 9.7340\n",
      "Epoch [2/5], Step[100/937], Loss: 17.1483\n",
      "Epoch [2/5], Step[200/937], Loss: 7.3881\n",
      "Epoch [2/5], Step[300/937], Loss: 6.2237\n",
      "Epoch [2/5], Step[400/937], Loss: 10.9434\n",
      "Epoch [2/5], Step[500/937], Loss: 12.4974\n",
      "Epoch [2/5], Step[600/937], Loss: 12.9216\n",
      "Epoch [2/5], Step[700/937], Loss: 16.8383\n",
      "Epoch [2/5], Step[800/937], Loss: 7.9035\n",
      "Epoch [2/5], Step[900/937], Loss: 14.2531\n",
      "Epoch [3/5], Step[100/937], Loss: 9.6237\n",
      "Epoch [3/5], Step[200/937], Loss: 7.8013\n",
      "Epoch [3/5], Step[300/937], Loss: 9.0709\n",
      "Epoch [3/5], Step[400/937], Loss: 8.0699\n",
      "Epoch [3/5], Step[500/937], Loss: 9.0526\n",
      "Epoch [3/5], Step[600/937], Loss: 4.8078\n",
      "Epoch [3/5], Step[700/937], Loss: 17.8962\n",
      "Epoch [3/5], Step[800/937], Loss: 10.7399\n",
      "Epoch [3/5], Step[900/937], Loss: 4.1891\n",
      "Epoch [4/5], Step[100/937], Loss: 11.1349\n",
      "Epoch [4/5], Step[200/937], Loss: 6.4548\n",
      "Epoch [4/5], Step[300/937], Loss: 5.7970\n",
      "Epoch [4/5], Step[400/937], Loss: 4.5484\n",
      "Epoch [4/5], Step[500/937], Loss: 13.9550\n",
      "Epoch [4/5], Step[600/937], Loss: 6.1578\n",
      "Epoch [4/5], Step[700/937], Loss: 5.5620\n",
      "Epoch [4/5], Step[800/937], Loss: 9.7412\n",
      "Epoch [4/5], Step[900/937], Loss: 6.5658\n",
      "Epoch [5/5], Step[100/937], Loss: 14.8984\n",
      "Epoch [5/5], Step[200/937], Loss: 6.8205\n",
      "Epoch [5/5], Step[300/937], Loss: 9.5789\n",
      "Epoch [5/5], Step[400/937], Loss: 7.4147\n",
      "Epoch [5/5], Step[500/937], Loss: 20.1357\n",
      "Epoch [5/5], Step[600/937], Loss: 6.4009\n",
      "Epoch [5/5], Step[700/937], Loss: 6.0563\n",
      "Epoch [5/5], Step[800/937], Loss: 3.4805\n",
      "Epoch [5/5], Step[900/937], Loss: 9.2407\n",
      "CPU times: user 2min 28s, sys: 99.2 ms, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch_iter in range(epochs):\n",
    "    for i,(data, target) in enumerate(train_loader):\n",
    "        x = data.to(device)\n",
    "        y = target.to(device)\n",
    "        spk_rec, mem_rec = net(x.view(batch_size, -1))\n",
    "        loss_val = torch.zeros((1),device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "             print (f'Epoch [{epoch_iter+1}/{epochs}], Step[{i+1}/{n_total_steps}], Loss: {loss_val.item():.4f}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, label in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(images.view(batch_size, -1))\n",
    "            n_samples += label.size(0)\n",
    "            _, idx = test_spk.sum(dim=0).max(1)\n",
    "            n_correct+= (label == idx).sum().item()\n",
    "            return(n_correct/n_samples * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.625"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'SNN_without_stashing.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating computational graph to see, whether all the neurons are contributing or not\n",
    "# batch = next(iter(train_loader))\n",
    "# yhat = net(batch[0].reshape(-1,28*28).to(device)) # Give dummy batch to forward().\n",
    "\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# make_dot(yhat, params=dict(list(net.named_parameters()))).render(\"trial_1\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
